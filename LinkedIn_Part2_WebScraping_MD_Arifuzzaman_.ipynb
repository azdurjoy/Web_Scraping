{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that Integrify wants to get some insights for the FullStack job market in order to build the best practice and update the curriculum to maximize the chance for getting as many job offers as possible for the students. \n",
    "Your tasks are the following:\n",
    "Each group member will be working on one country [Finland, Netherlands, Denmark, Sweden, and Germany]\n",
    "Use the following keyword sets and try to locate 20 companies in each country:\n",
    "FS= [Front-end development, HTML, CSS, JavaScript, React, Angular, Vue.js, Bootstrap, jQuery, responsive design, Back-end development, Node.js, Python, Ruby, PHP, Java, .NET, SQL, NoSQL, RESTful APIs, web servers, Database management,  MySQL, PostgreSQL, MongoDB, Redis, Cassandra, Oracle, SQL Server, DevOps, AWS, Azure, Google Cloud, Docker, Kubernetes, Git, Jenkins, Travis CI, CircleCI, monitoring and logging tools, Project management, Agile, Scrum, Kanban, JIRA, Trello, Asana, project planning, team collaboration, communication skills]\n",
    "\n",
    "Collect all job offers of each company for a one-year time frame. \n",
    "You will end up with a dictionary where the keys are the company names and the values are a list of dictionaries. \n",
    "The keys in the sub-dictionaries correspond to keywords, and the values correspond to the company’s posts that include those keywords. \n",
    "In total, you will produce five dictionaries, each corresponding to one of the listed countries above. \n",
    "Save each dictionary in JSON format under the name of the corresponding country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class LinkedinScraper:\n",
    "    keywords = ['Front-end development', 'HTML', 'CSS', 'JavaScript', 'React', 'Angular', 'Vue.js', 'Bootstrap', 'jQuery', 'responsive design',\n",
    "                'Back-end development', 'Node.js', 'Python', 'Ruby', 'PHP', 'Java', '.NET', 'SQL', 'NoSQL', 'RESTful APIs', 'web servers',\n",
    "                'Database management', 'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Cassandra', 'Oracle', 'SQL Server',\n",
    "                'DevOps', 'AWS', 'Azure', 'Google Cloud', 'Docker', 'Kubernetes', 'Git', 'Jenkins', 'Travis CI', 'CircleCI', 'monitoring and logging tools',\n",
    "                'Project management', 'Agile', 'Scrum', 'Kanban', 'JIRA', 'Trello', 'Asana', 'project planning', 'team collaboration', 'communication skills']\n",
    "\n",
    "    def __init__(self, country_name, geoId, companies):\n",
    "        self.country_name = country_name\n",
    "        self.geoId = geoId\n",
    "        self.companies = companies\n",
    "        self.search_url_pattern = 'https://www.linkedin.com/jobs/search/?currentJobId={}&distance=25&geoId={}&keywords={}&refresh=true&start={}'\n",
    "\n",
    "    def scrape_jobs(self, output_file_path):\n",
    "        job_list = []\n",
    "        for i, company in enumerate(self.companies):\n",
    "            print(f\"Company {i+1}: {company}\")\n",
    "            start_index = 0\n",
    "\n",
    "            while True:\n",
    "                search_url = self.search_url_pattern.format(start_index, self.geoId, company, start_index)\n",
    "                response = requests.get(search_url)\n",
    "\n",
    "                # Parse the HTML content of the page using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Find all the job posting URLs on the page\n",
    "                job_urls = [a['href'] for a in soup.find_all('a', href=True) if '/jobs/view/' in a['href']]\n",
    "\n",
    "                # If no matching jobs are found, break the loop\n",
    "                if len(job_urls) == 0:\n",
    "                    break\n",
    "\n",
    "                # Iterate over each job URL and extract the job information\n",
    "                for job_url in job_urls:\n",
    "                    # Send a GET request to the job URL\n",
    "                    response = requests.get(job_url)\n",
    "\n",
    "                    # Parse the HTML content of the page using BeautifulSoup\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Find the job title and company name\n",
    "                    title = soup.find('h1', class_='topcard__title')\n",
    "                    if title is not None:\n",
    "                        title = title.text.strip()\n",
    "                    else:\n",
    "                        continue\n",
    "                    company_name = soup.find('a', class_='topcard__org-name-link')\n",
    "                    if company_name is not None:\n",
    "                        company_name = company_name.text.strip()\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    # Find the div tag that contains the job description\n",
    "                    description_div = soup.find('div', class_='description__text')\n",
    "\n",
    "                    if description_div is not None:\n",
    "                        # Extract the job description text\n",
    "                        job_description = description_div.get_text().strip()\n",
    "                        job_description = job_description.replace('Show more', '').replace('Show less', '')\n",
    "\n",
    "                                                # Remove any extra spaces from the job description\n",
    "                        job_description = ' '.join(job_description.split())\n",
    "\n",
    "                        # Find the keywords present in the job description\n",
    "                        found_keywords = [keyword for keyword in self.keywords if keyword in job_description]\n",
    "\n",
    "                        # Create a dictionary with job data\n",
    "                        job_data = {\n",
    "                            'Country_name': self.country_name,\n",
    "                            'company_name': company_name,\n",
    "                            'keywords': found_keywords,\n",
    "                            'description': job_description\n",
    "                        }\n",
    "\n",
    "                        job_list.append(job_data)\n",
    "                        print(\"Job Title:\", title)\n",
    "                        print(\"Company Name:\", company_name)\n",
    "                        print(\"Keywords:\", found_keywords)\n",
    "                        print(\"Description:\", job_description)\n",
    "                        print()\n",
    "\n",
    "                    start_index += len(job_urls)\n",
    "\n",
    "        # Write the job data to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(job_list, json_file, indent=4)\n",
    "\n",
    "# Example usage : Finland\n",
    "# country_name = \"Finland\"\n",
    "# geoId = \"100456013\"\n",
    "# companies = ['Oliver Parks', 'Trimble Inc.', 'Knowit', 'Eficode', 'Wolt', 'Nigel Frank International', 'Tietoevry', 'Nortal', 'Silo AI', 'Solita', 'CGI', 'Accenture', 'Wärtsilä', 'F-Secure', 'Neste', 'Nordea', 'Aiven', 'Unikie', 'Smartly.io', 'Nokia', 'Vaisala', 'Futurice', 'Elisa', 'Taiste', 'AlphaSense', 'Gofore', 'Canonical', 'Vincit']\n",
    "\n",
    "# scraper = LinkedinScraper(country_name, geoId, companies)\n",
    "# scraper.scrape_jobs(\"jobs_finland.json\")\n",
    "\n",
    "#Example usage : Norway\n",
    "# country_name = \"Norway\"\n",
    "# geoId = \"103819153\"\n",
    "# companies = ['Equinor', 'Viddal Automation AS', 'Appen', 'StaffHost Europe', 'Posten Norge AS', 'Accenture Nordics', 'PA Consulting', 'Capgemini', 'Ving Norge AS', 'Nexere Consulting', 'Dun & Bradstreet Europe', 'Yara International', 'Nielsen', 'SpareBank 1 SMN', 'Vegfinans', 'TOMRA', 'Fugro']\n",
    "# scraper = LinkedinScraper(country_name, geoId, companies)\n",
    "# scraper.scrape_jobs(\"jobs_Norway.json\")\n",
    "\n",
    "#Example usage : Germany\n",
    "country_name = \"Germany\"\n",
    "geoId = \"101282230\"\n",
    "companies = ['xValue GmbH', 'kloeckner.i GmbH', 'Gerhard Schubert GmbH Verpackungsmaschinen', 'Renesas Electronics', 'GIANT-HR Mittelstandsberatung GmbH', 'RITTERWALD Unternehmensberatung GmbH', 'Axelera AI', 'AILY LABS', 'Hypatos', 'Boehringer Ingelheim', 'neurocat', 'ACST GmbH', 'TES-H2', 'BwFuhrparkService GmbH', 'MEIKO Group', 'Lufthansa Technik', 'Fraunhofer Karriere', 'Liebherr Group', 'Genova.ai']\n",
    "scraper = LinkedinScraper(country_name, geoId, companies)\n",
    "scraper.scrape_jobs(\"jobs_Germany.json\")\n",
    "\n",
    "#Example usage : Denmark\n",
    "# country_name = \"Denmark\"\n",
    "# geoId = \"104514075\"\n",
    "# companies = ['Cognizant', 'Vestas', 'EIVEE™', 'Radiobotics', 'HelloFresh', 'Kiloo Games', 'Ipsos', 'TotalEnergies', 'Elos Medtech', 'GEA Group', 'Siemens Gamesa', 'SimCorp']\n",
    "# scraper = LinkedinScraper(country_name, geoId, companies)\n",
    "# scraper.scrape_jobs(\"jobs_Denmark.json\")\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb128f72f65f49a14308e9d53339f1d3298c55f21eccb341966cf41e8ec217b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
